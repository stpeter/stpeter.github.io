<journal>
<header><title>The Primacy of the Prompt</title><date>2023-11-16</date></header>
<entry>
<p>While re-reading Robert Sokolowski's excellent book <cite>Phenomenology of the Human Person</cite> recently, I came across a fascinating quote from Derek Bickerton's book <cite>Language and Human Behavior</cite>:</p>
<blockquote>
The problems animals solve, the problems we solve, are our <em>own</em> problems.... But the problems computers solve are not problems for computers. If I have a problem, it's my problem. If my computer has a problem, it's still my problem. Nothing is a problem for it, because it doesn't interact with the world. It just sits there and waits for me to give it <em>my</em> problems.
</blockquote>
<p>This is why LLMs like ChatGPT have a prompt. The LLM doesn't figuratively or literally walk into my office and say "hey, I've been thinking about this problem I've got and I'd like to bounce around some potential solutions with you"; in the terms I introduced a few weeks ago, the LLM isn't <em>sentient</em> because nothing is <em>salient</em> for it. For this reason, in contrast with folks who are excited about the prospect of artificial intelligence, I find these systems to be fundamentally boring.</p>
<p>(Cross-posted at <a href='https://philosopher.coach/'>philosopher.coach</a>.)</p>
<p><em>FOR FURTHER EXPLORATION</em></p>
<ul>
<li><a href='1825.html'>Sentience and Salience (2023-09-30)</a></li>
</ul>
</entry>
</journal>
